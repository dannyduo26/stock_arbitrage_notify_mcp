import sys
import json
import re
import time
from typing import List, Dict, Any

try:
    import httpx  # type: ignore
except Exception:
    httpx = None  # type: ignore

try:
    from bs4 import BeautifulSoup  # type: ignore
except Exception:
    BeautifulSoup = None  # type: ignore


try:
    from mcp.server.fastmcp import FastMCP  # type: ignore
except Exception:
    FastMCP = None  # type: ignore


URL = "https://www.jisilu.cn/data/qdii/#qdiie"


def _to_float_percent(s: str) -> float:
    # 将百分数字符串转为浮点数（去掉%和+号）
    s = s.strip().replace("%", "").replace("+", "")
    try:
        return float(s)
    except Exception:
        return float("nan")


def _fetch_html(url: str) -> str:
    # 简单HTTP抓取，优先使用httpx，失败时回退到urllib
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Referer": "https://www.jisilu.cn/data/qdii/",
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    if httpx is not None:
        with httpx.Client(timeout=20) as client:
            resp = client.get(url, headers=headers)
            resp.raise_for_status()
            return resp.text
    import urllib.request

    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=20) as resp:
        return resp.read().decode("utf-8", errors="ignore")


def _parse_with_bs4(html: str) -> List[Dict[str, Any]]:
    # 使用BeautifulSoup解析表格，提取 T-1溢价率 与 申购状态 等字段
    try:
        from lxml import html as lxml_html  # type: ignore
    except Exception:
        return []
    try:
        tree = lxml_html.fromstring(html)
    except Exception:
        return []
    nodes = tree.xpath('//td[@data-name="apply_status"]/text()')
    result: List[Dict[str, Any]] = []
    for txt in nodes:
        s = str(txt).strip()
        if s:
            result.append({"申购状态": s})
    return result


def _parse_with_regex(html: str) -> List[Dict[str, Any]]:
    # 当表格解析失败时，使用正则从纯文本回退提取核心字段
    text = re.sub(r"<[^>]+>", " ", html)
    text = re.sub(r"\s+", " ", text)
    chunks = re.split(r"(?=\b\d{6}\b)", text)
    result: List[Dict[str, Any]] = []
    for chunk in chunks:
        m_code = re.search(r"\b(\d{6})\b", chunk)
        if not m_code:
            continue
        code = m_code.group(1)
        m_name = re.search(r"\b\d{6}\b\s*([^\d%\-]{2,}?)\s", chunk)
        name = m_name.group(1).strip() if m_name else ""
        m_t1 = re.search(r"T-1溢价率\s*([+\-]?[\d\.]+)%", chunk)
        t1 = m_t1.group(1) + "%" if m_t1 else ""
        m_sub = re.search(r"申购状态\s*([\u4e00-\u9fffA-Za-z0-9%]+)", chunk)
        sub = m_sub.group(1) if m_sub else ""
        if t1 or sub:
            result.append({"代码": code, "名称": name, "T-1溢价率": t1, "申购状态": sub})
    return result


def _fetch_api_rows() -> List[Dict[str, Any]]:
    """从集思录 API 获取数据，包括 QDII 和 LOF 基金"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Referer": "https://www.jisilu.cn/data/qdii/",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    out: List[Dict[str, Any]] = []
    
    # 1. 获取 QDII 数据
    for cat in ["E", "C", "A"]:
        url = f"https://www.jisilu.cn/data/qdii/qdii_list/{cat}"
        params = {"___jsl": f"LST___t={int(time.time()*1000)}", "rp": "22"}
        if cat in ("E", "A"):
            params.update({"only_lof": "y", "only_etf": "y"})
        try:
            data = None
            if httpx is not None:
                resp = httpx.get(url, params=params, headers=headers, timeout=20)
                resp.raise_for_status()
                data = resp.json()
            else:
                import urllib.parse
                import urllib.request
                q = urllib.parse.urlencode(params)
                req = urllib.request.Request(url + "?" + q, headers=headers)
                with urllib.request.urlopen(req, timeout=20) as f:
                    data = json.loads(f.read().decode("utf-8", errors="ignore"))
        except Exception:
            data = None
        if not isinstance(data, dict):
            continue
        for row in data.get("rows", []):
            cell = row.get("cell", {})
            out.append({
                "代码": str(cell.get("fund_id", "")),
                "名称": str(cell.get("fund_nm", "")),
                "T-1溢价率": str(cell.get("discount_rt", "")),
                "申购状态": str(cell.get("apply_status", "")),
            })
    
    # 2. 获取 LOF 数据
    lof_url = "https://www.jisilu.cn/data/lof/index_lof_list/"
    lof_params = {
        "___jsl": f"LST___t={int(time.time()*1000)}",
        "rp": "25",
        "page": "1"
    }
    try:
        data = None
        if httpx is not None:
            resp = httpx.get(lof_url, params=lof_params, headers=headers, timeout=20)
            resp.raise_for_status()
            data = resp.json()
        else:
            import urllib.parse
            import urllib.request
            q = urllib.parse.urlencode(lof_params)
            req = urllib.request.Request(lof_url + "?" + q, headers=headers)
            with urllib.request.urlopen(req, timeout=20) as f:
                data = json.loads(f.read().decode("utf-8", errors="ignore"))
        
        if isinstance(data, dict):
            for row in data.get("rows", []):
                cell = row.get("cell", {})
                # LOF 数据格式可能与 QDII 略有不同，需要适配
                out.append({
                    "代码": str(cell.get("fund_id", "")),
                    "名称": str(cell.get("fund_nm", "")),
                    "T-1溢价率": str(cell.get("discount_rt", "")),
                    "申购状态": str(cell.get("apply_status", "")),
                })
    except Exception as e:
        # LOF 数据获取失败时不影响整体流程，记录错误但继续
        pass
    
    return out


def _fetch_ak_rows() -> List[Dict[str, Any]]:
    try:
        import akshare as ak  # type: ignore
    except Exception:
        return []
    names = [
        "qdii_e_index_jsl",
        "qdii_e_comm_jsl",
        "qdii_c_jsl",
        "qdii_a_jsl",
    ]
    datasets: List[Any] = []
    for n in names:
        try:
            fn = getattr(ak, n, None)
            if callable(fn):
                df = fn()
                datasets.append(df)
        except Exception:
            continue
    out: List[Dict[str, Any]] = []
    for df in datasets:
        try:
            for _, r in df.iterrows():
                out.append({
                    "代码": str(r.get("代码", r.get("fund_id", ""))),
                    "名称": str(r.get("名称", r.get("fund_nm", ""))),
                    "T-1溢价率": str(r.get("T-1溢价率", r.get("T-1 溢价率", r.get("discount_rt", "")))),
                    "申购状态": str(r.get("申购状态", r.get("apply_status", ""))),
                })
        except Exception:
            continue
    return out

def _fetch_data() -> List[Dict[str, Any]]:
    rows = _fetch_api_rows()
    if rows:
        return rows
    return _fetch_ak_rows()


def qdii_candidates(threshold: float = 2.0) -> List[Dict[str, Any]]:
    # 过滤逻辑：T-1溢价率 > threshold 且 申购状态 ≠ "暂停申购" 且 申购状态 ≠ "开放申购"
    rows = _fetch_data()
    out: List[Dict[str, Any]] = []
    for r in rows:
        p = _to_float_percent(str(r.get("T-1溢价率", "")))
        status = str(r.get("申购状态", ""))
        # if p == p and p > threshold and status.startswith("限"):
        if p == p and p > threshold and status != "暂停申购" and status != "开放申购":
            out.append({
                "代码": r.get("代码", ""),
                "名称": r.get("名称", ""),
                "T-1溢价率": p,
                "申购状态": status,
            })
    return out

if __name__ == "__main__":
    res = qdii_candidates(2.0)
    print(json.dumps(res, ensure_ascii=False, indent=2))
# MCP服务器：抓取集思录QDII页面数据，筛选满足溢价与申购条件的基金
# 提供工具 fetch_qdii_candidates 供外部通过MCP调用